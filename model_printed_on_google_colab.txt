LlamaForTraining(
  (model): LlamaModel(
    (token_position_embeddings): PipelineBlock(
      pp_rank=0
      (pp_block): Embedding(
        (token_embedding): TensorParallelEmbedding(tp_rank=0, 49152, 576, unsharded_num_embeddings=49152)
      )
    )
    (decoder): ModuleList(
      (0-29): 30 x PipelineBlock(
        pp_rank=0
        (pp_block): LlamaDecoderLayer(
          (input_layernorm): TritonRMSNorm()
          (attn): CausalSelfAttention(
            (qkv_proj): TensorParallelColumnLinear(tp_rank=0, in_features=576, out_features=960, bias=False, unsharded_out_features=960)
            (rotary_embedding): LlamaRotaryEmbedding()
            (flash_rotary_embedding): RotaryEmbedding()
            (o_proj): TensorParallelRowLinear(tp_rank=0, in_features=576, out_features=576, bias=False, unsharded_in_features=576)
            (attention): CoreAttention()
          )
          (post_attention_layernorm): TritonRMSNorm()
          (mlp): MLP(
            (gate_up_proj): TensorParallelColumnLinear(tp_rank=0, in_features=576, out_features=3072, bias=False, unsharded_out_features=3072)
            (down_proj): TensorParallelRowLinear(tp_rank=0, in_features=1536, out_features=576, bias=False, unsharded_in_features=1536)
            (split_silu_mul): GLUActivation(
              (act): SiLUActivation()
            )
          )
        )
      )
    )
    (final_layer_norm): PipelineBlock(
      pp_rank=0
      (pp_block): TritonRMSNorm()
    )
    (lm_head): PipelineBlock(
      pp_rank=0
      (pp_block): TensorParallelColumnLinear(tp_rank=0, in_features=576, out_features=49152, bias=False, unsharded_out_features=49152)
    )
    (cast_to_fp32): PipelineBlock(pp_rank=0)
  )
  (loss): PipelineBlock(
    pp_rank=0
    (pp_block): Loss()
  )
)